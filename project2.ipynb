{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 18:15:58 WARN Utils: Your hostname, codespaces-38b548 resolves to a loopback address: 127.0.0.1; using 10.0.4.31 instead (on interface eth0)\n",
      "24/11/19 18:15:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 18:15:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Project2\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "output_dir = \"output/\"\n",
    "task2_output_total = output_dir + \"task2/task2_total.csv\"\n",
    "task2_output_avgsales = output_dir + \"task2/task2_avgsales.csv\"\n",
    "task2_output_season = output_dir + \"task2/task2_season.csv\"\n",
    "task3_output = output_dir + \"task3/task3.csv\"\n",
    "task4_output = output_dir + \"task4/task4.csv\"\n",
    "\n",
    "checkpoint_dir = \"checkpoint/task5/\"\n",
    "task5_output = output_dir + \"task5.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "|      asin|helpful|      main_image_url|       product_title|            sentence|\n",
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "|B000AO3L84|    1.7|http://ecx.images...|Canon 430EX Speed...|this flash is a s...|\n",
      "|B001SEQPGK|    1.3|http://ecx.images...|Sony Cyber-shot D...|The pictures were...|\n",
      "|0553386697|    1.9|http://ecx.images...|The Whole-Brain C...|A very good resou...|\n",
      "|B006SUWZH2|   0.25|http://ecx.images...|Memorex Portable ...|We have it in a c...|\n",
      "|B000W7F5SS|    0.9|http://ecx.images...|Harry Potter and ...|Again the makers ...|\n",
      "|B000AO3L84|    2.0|http://ecx.images...|Canon 430EX Speed...|This flash is a g...|\n",
      "|B00081NX5U|   0.73|http://ecx.images...|iPod Detachable R...|So I've had these...|\n",
      "|B00000F1D3|    0.9|http://ecx.images...|             Believe|they're cd's or t...|\n",
      "|B00000FCBH|    1.3|http://ecx.images...|  2Pac Greatest Hits|he proved that ev...|\n",
      "|B00013M6NU|    0.4|http://ecx.images...|Nikon MH-61 Batte...|I realize these t...|\n",
      "|0375703764|    1.5|http://ecx.images...|     House of Leaves|Emulate contortio...|\n",
      "|B000002GJH|    0.9|http://ecx.images...|   Temple Of The Dog|The best tracks i...|\n",
      "|B002JSM3KQ|    0.2|http://ecx.images...|Monopoly Board Ga...|, so Amazon likes...|\n",
      "|B008FSCNTK|    0.4|http://ecx.images...|       Glad Rag Doll|but I'll be check...|\n",
      "|B003T90WY8|    1.7|http://ecx.images...|EARBUDi Clips on ...|Edit to add: Thes...|\n",
      "|B00002E220|    0.8|http://ecx.images...|  Last of the Dogmen|I had the VHS and...|\n",
      "|B0002COTDA|    1.0|http://ecx.images...|Columbo - The Com...|Columbo is one of...|\n",
      "|0061208493|   1.36|http://ecx.images...|The Complete C. S...|any of his books ...|\n",
      "|B000002KB8|    1.0|http://ecx.images...|       Black Sabbath|The title has not...|\n",
      "|0470474874|    1.1|http://ecx.images...|    Excel 2010 Bible|It basically copi...|\n",
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.json(\"train.json\")\n",
    "reviwes_df = spark.read.json(\"test.json\")\n",
    "train_df.show()\n",
    "#reviwes_df = reviwes_df.withColumn(\"helpful\", F.col(\"helpful\").cast(\"double\"))\n",
    "#reviwes_df.printSchema()\n",
    "#print(reviwes_df.describe())\n",
    "# Check if any rows have nulls in the 'helpful' column after casting\n",
    "#invalid_rows = reviwes_df.filter(F.col(\"sentence\").isNull())\n",
    "# Show rows that could not be cast to double\n",
    "#invalid_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "|      asin|helpful|      main_image_url|       product_title|            sentence|\n",
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "|B00VG90446|   1.07|http://ecx.images...|Flexion KS-902 Ki...|so it stays in pl...|\n",
      "|B001196MG0|   1.33|http://ecx.images...|Savage 107X12-1 S...|Love this seamles...|\n",
      "|B00081NX5U|   1.17|http://ecx.images...|iPod Detachable R...|very happy with m...|\n",
      "|B003HC9JIW|    1.6|http://ecx.images...|Start! Walking At...|Even for someone ...|\n",
      "|B00C30FCUI|   1.49|http://ecx.images...|Symphonized NRG P...|, those have alwa...|\n",
      "|B001196MG0|   1.47|http://ecx.images...|Savage 107X12-1 S...|but after a year ...|\n",
      "|B00AR1G3FS|   1.24|http://ecx.images...|Farewell Live Fro...|While not quite a...|\n",
      "|B007R3AZNK|   0.67|http://ecx.images...|Driving Towards T...|Until now, Sloe G...|\n",
      "|0761165975|    1.0|http://ecx.images...|The Wedding Plann...|I considered the ...|\n",
      "|B00000FCBH|   0.03|http://ecx.images...|  2Pac Greatest Hits|\"Baby, don't cry,...|\n",
      "|B000HKDE7Y|   0.74|http://ecx.images...|Struggle From The...|I'm glad I bought...|\n",
      "|B00081NX5U|   1.59|http://ecx.images...|iPod Detachable R...|Great sound I bou...|\n",
      "|0451232852|    0.5|http://ecx.images...|Fall of Giants: B...|The Age of Enligh...|\n",
      "|B000AO3L84|   1.66|http://ecx.images...|Canon 430EX Speed...|When you take pic...|\n",
      "|B000002IAO|   0.75|http://ecx.images...|  In-A-Gadda-Da-Vida|There were times ...|\n",
      "|B000002GJH|   0.37|http://ecx.images...|   Temple Of The Dog|Grunge can't just...|\n",
      "|0671015206|   1.03|http://ecx.images...|The Millionaire N...|Worth Skipping, o...|\n",
      "|0670012335|   0.31|http://ecx.images...|Llama Llama Time ...|This was a Christ...|\n",
      "|B0019FHM9M|   0.39|http://ecx.images...|Lowpricenice MH20...|-----------------...|\n",
      "|0399162089|   1.13|http://ecx.images...|Outside the Lines...|I couldn't make t...|\n",
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviwes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "online_retail_df = spark.read.csv(\"Online-Retail.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|       1454|       0|          0|        0|    135080|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "online_retail_df.select([F.count(F.when(col(c).isNull(), c)).alias(c) for c in online_retail_df.columns]).show()\n",
    "\n",
    "# Drop rows with nulls in important columns like CustomerID, Description, etc.\n",
    "online_retail_df = online_retail_df.dropna(subset=[\"CustomerID\", \"Description\", \"InvoiceDate\", \"Quantity\", \"UnitPrice\"])\n",
    "#online_retail_df.select([F.count(F.when(col(c).isNull(), c)).alias(c) for c in online_retail_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "online_retail_df = online_retail_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast the invoice date to timestamp\n",
    "online_retail_df = online_retail_df.withColumn(\"InvoiceDate\", F.to_timestamp(\"InvoiceDate\", \"M/d/yyyy H:mm\"))\n",
    "online_retail_df = online_retail_df.withColumn(\"InvoiceDate\", F.col(\"InvoiceDate\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|  C538350|   85099F|JUMBO BAG STRAWBERRY|      -2|2010-12-10 15:01:00|     1.65|     13798|United Kingdom|\n",
      "|  C538375|    22220|CAKE STAND LOVEBI...|      -1|2010-12-12 11:19:00|     9.95|     17126|United Kingdom|\n",
      "|  C539726|    22791|T-LIGHT GLASS FLU...|     -10|2010-12-21 14:24:00|     1.25|     17007|United Kingdom|\n",
      "|  C540307|    22084|PAPER CHAIN KIT E...|     -36|2011-01-06 12:58:00|     2.95|     15823|United Kingdom|\n",
      "|  C542138|    20866|BLUE ROSE FABRIC ...|    -120|2011-01-25 17:21:00|     1.06|     17368|United Kingdom|\n",
      "|  C543347|    22629| SPACEBOY LUNCH BOX |      -1|2011-02-07 12:44:00|     1.95|     12472|       Germany|\n",
      "|  C544830|    22059|CERAMIC STRAWBERR...|      -6|2011-02-24 09:59:00|     1.49|     13118|United Kingdom|\n",
      "|  C545456|    21527|RED RETROSPOT TRA...|      -1|2011-03-02 17:15:00|     7.95|     17865|United Kingdom|\n",
      "|  C545773|    21843|RED RETROSPOT CAK...|      -1|2011-03-07 12:06:00|    10.95|     17139|United Kingdom|\n",
      "|  C546168|    22892|SET OF SALT AND P...|     -12|2011-03-10 10:22:00|     1.25|     16670|United Kingdom|\n",
      "|  C546511|    22801|ANTIQUE GLASS PED...|      -1|2011-03-14 12:19:00|     3.75|     12921|United Kingdom|\n",
      "|  C548015|    82583|HOT BATHS METAL SIGN|     -30|2011-03-29 11:30:00|     1.69|     13802|United Kingdom|\n",
      "|  C548187|    22423|REGENCY CAKESTAND...|      -1|2011-03-29 15:06:00|    12.75|     17511|United Kingdom|\n",
      "|  C542642|    22720|SET OF 3 CAKE TIN...|      -1|2011-01-31 11:27:00|     4.95|     15827|United Kingdom|\n",
      "|  C543175|     POST|             POSTAGE|      -1|2011-02-04 09:46:00|     18.0|     12712|       Germany|\n",
      "|  C543918|    71459|HANGING JAM JAR T...|      -5|2011-02-14 13:42:00|     0.85|     12921|United Kingdom|\n",
      "|  C544655|    37447|CERAMIC CAKE DESI...|      -5|2011-02-22 12:57:00|     1.49|     13113|United Kingdom|\n",
      "|  C546730|    21937|STRAWBERRY   PICN...|      -2|2011-03-16 11:39:00|     2.95|     18183|United Kingdom|\n",
      "|  C547930|     POST|             POSTAGE|      -1|2011-03-28 14:00:00|     18.0|     12594|         Italy|\n",
      "|  C548454|        M|              Manual|      -1|2011-03-31 11:42:00|     57.6|     16422|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#check for outliers\n",
    "online_retail_df.filter((col(\"Quantity\") < 0) | (col(\"UnitPrice\") < 0)).show()\n",
    "\n",
    "# Drop rows where Quantity or UnitPrice are negative (common outlier check)\n",
    "online_retail_df = online_retail_df.filter((F.col(\"Quantity\") >= 0) & (F.col(\"UnitPrice\") >= 0))\n",
    "#online_retail_df.filter((col(\"Quantity\") < 0) | (col(\"UnitPrice\") < 0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize for consistent description characters\n",
    "online_retail_df = online_retail_df.withColumn(\"Description\", F.upper(F.col(\"Description\")))\n",
    "\n",
    "#handle special cases in case for better performance for MLlib\n",
    "#online_retail_df = online_retail_df.filter(~F.col(\"InvoiceNo\").startswith(\"C\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|     13047|United Kingdom|\n",
      "|   536368|    22960|JAM MAKING SET WI...|       6|2010-12-01 08:34:00|     4.25|     13047|United Kingdom|\n",
      "|   536388|    22915|ASSORTED BOTTLE T...|      12|2010-12-01 09:59:00|     0.42|     16250|United Kingdom|\n",
      "|   536401|    21464|DISCO BALL ROTATO...|       1|2010-12-01 11:21:00|     4.25|     15862|United Kingdom|\n",
      "|   536412|    22569|FELTCRAFT CUSHION...|       2|2010-12-01 11:49:00|     3.75|     17920|United Kingdom|\n",
      "|   536425|    22645|CERAMIC HEART FAI...|      12|2010-12-01 12:08:00|     1.45|     13758|United Kingdom|\n",
      "|   536488|    22376|AIRLINE BAG VINTA...|       1|2010-12-01 12:31:00|     4.25|     17897|United Kingdom|\n",
      "|   536520|    21930|JUMBO STORAGE BAG...|       1|2010-12-01 12:43:00|     1.95|     14729|United Kingdom|\n",
      "|   536534|    22866|HAND WARMER SCOTT...|      12|2010-12-01 13:33:00|      2.1|     15350|United Kingdom|\n",
      "|   536540|   85136A|YELLOW SHARK HELI...|       2|2010-12-01 14:05:00|     7.95|     14911|          EIRE|\n",
      "|   536562|   79302M|ART LIGHTS,FUNK M...|       6|2010-12-01 15:08:00|     2.95|     13468|United Kingdom|\n",
      "|   536569|    22941|CHRISTMAS LIGHTS ...|       1|2010-12-01 15:35:00|      8.5|     16274|United Kingdom|\n",
      "|   536571|    21352|EUCALYPTUS & PINE...|       2|2010-12-01 15:37:00|     6.75|     14696|United Kingdom|\n",
      "|   536591|    21985|PACK OF 12 HEARTS...|       4|2010-12-01 16:58:00|     0.29|     14606|United Kingdom|\n",
      "|   536624|    22672|FRENCH BATHROOM S...|      12|2010-12-02 10:45:00|     1.65|     13418|United Kingdom|\n",
      "|   536624|    21843|RED RETROSPOT CAK...|       4|2010-12-02 10:45:00|    10.95|     13418|United Kingdom|\n",
      "|   536630|    22752|SET 7 BABUSHKA NE...|       2|2010-12-02 10:56:00|     7.65|     17850|United Kingdom|\n",
      "|   536635|    22441|GROW YOUR OWN BAS...|       8|2010-12-02 11:22:00|      2.1|     15955|United Kingdom|\n",
      "|   536667|    22594|CHRISTMAS GINGHAM...|      24|2010-12-02 12:09:00|     0.85|     15260|United Kingdom|\n",
      "|   536671|    22740|        POLKADOT PEN|      48|2010-12-02 12:10:00|     0.85|     13305|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "online_retail_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----+----+------------------+\n",
      "|StockCode|         Description|Month|Year|        TotalSales|\n",
      "+---------+--------------------+-----+----+------------------+\n",
      "|    22692|DOORMAT WELCOME T...|    4|2011|             512.7|\n",
      "|    22384|LUNCH BAG PINK PO...|    1|2011|            902.15|\n",
      "|    21221|SET/4 BADGES CUTE...|    2|2011|             43.75|\n",
      "|    22114|HOT WATER BOTTLE ...|   12|2010|1863.0500000000002|\n",
      "|    22236|CAKE STAND 3 TIER...|    1|2011|403.04999999999995|\n",
      "|    85213|MINI  ZINC GARDEN...|    3|2011|44.199999999999996|\n",
      "|    22624|IVORY KITCHEN SCALES|    3|2011|            779.45|\n",
      "|    20914|SET/5 RED RETROSP...|    3|2011| 962.6500000000001|\n",
      "|    21051|      RIBBONS PURSE |   12|2010|              33.6|\n",
      "|    21989|PACK OF 20 SKULL ...|    1|2011|115.59999999999998|\n",
      "|    21615|4 LAVENDER BOTANI...|    1|2011|             165.0|\n",
      "|    22807|SET OF 6 T-LIGHTS...|   12|2010|141.60000000000002|\n",
      "|    21888|           BINGO SET|    2|2011|             127.5|\n",
      "|   84531B|BLUE KNITTED EGG ...|    3|2011|40.949999999999996|\n",
      "|    22026|BANQUET BIRTHDAY ...|    2|2011|             66.24|\n",
      "|    22976|CIRCUS PARADE CHI...|    2|2011|             98.75|\n",
      "|    22079|RIBBON REEL HEART...|    3|2011|             211.2|\n",
      "|    22781|GUMBALL MAGAZINE ...|   12|2010|            267.75|\n",
      "|    22474|SPACEBOY TV DINNE...|    3|2011|59.400000000000006|\n",
      "|    21967|PACK OF 12 SKULL ...|   12|2010| 92.50999999999999|\n",
      "+---------+--------------------+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "#Task 2: Sales Data Aggregation and Feature Engineering\n",
    "#-----------------------------------\n",
    "#total sales per product per month\n",
    "#get the month and year from invoicedate\n",
    "online_retail_df = online_retail_df.withColumn(\"Month\", F.month(\"InvoiceDate\")).withColumn(\"Year\", F.year(\"InvoiceDate\"))\n",
    "\n",
    "#make a revenue column, total quantity * price\n",
    "online_retail_df = online_retail_df.withColumn(\"Revenue\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
    "\n",
    "#total sales per product and month. calculated by summing total quantity * price\n",
    "total_sales_df = online_retail_df.groupBy(\"StockCode\", \"Description\", \"Month\", \"Year\").agg(F.sum(\"Revenue\").alias(\"TotalSales\"))\n",
    "total_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 18:16:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 21:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|    AverageRevenue|\n",
      "+----------+------------------+\n",
      "|     12346|           77183.6|\n",
      "|     16446|           56157.5|\n",
      "|     15098|           13305.5|\n",
      "|     15749|           4453.43|\n",
      "|     15195|            3861.0|\n",
      "|     13135|            3096.0|\n",
      "|     17846|            2033.1|\n",
      "|     18087|2027.8599999999997|\n",
      "|     16532|1687.1999999999998|\n",
      "|     16000|1377.0777777777776|\n",
      "|     16754|            1001.2|\n",
      "|     12755| 952.9874999999998|\n",
      "|     18133| 931.4999999999999|\n",
      "|     12798| 872.1299999999999|\n",
      "|     17949|           835.864|\n",
      "|     17553|             743.8|\n",
      "|     15299| 643.8585714285715|\n",
      "|     16308|             640.0|\n",
      "|     16986|             624.4|\n",
      "|     18080|            615.75|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#average revenue per customer\n",
    "#get average revnue for each customer id\n",
    "avg_revnue_df = online_retail_df.groupBy(\"CustomerID\").agg(F.avg(\"Revenue\").alias(\"AverageRevenue\")).orderBy(F.desc(\"AverageRevenue\"))\n",
    "avg_revnue_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----+------------------+\n",
      "|StockCode|         Description|Month|      MonthlySales|\n",
      "+---------+--------------------+-----+------------------+\n",
      "|   85123A|WHITE HANGING HEA...|    4| 9581.650000000001|\n",
      "|    84879|ASSORTED COLOUR B...|    1|2704.1899999999996|\n",
      "|    23166|MEDIUM CERAMIC TO...|    9|            397.26|\n",
      "|    47566|       PARTY BUNTING|    1|1815.1499999999999|\n",
      "|        M|              MANUAL|    8|           2989.54|\n",
      "|   85123A|WHITE HANGING HEA...|    2|4912.6500000000015|\n",
      "|   85099B|JUMBO BAG RED RET...|   10| 9763.059999999998|\n",
      "|    22423|REGENCY CAKESTAND...|    1|10765.499999999998|\n",
      "|   85099B|JUMBO BAG RED RET...|    7| 5654.599999999999|\n",
      "|     POST|             POSTAGE|   11|          10349.95|\n",
      "|    47566|       PARTY BUNTING|   11|3715.7099999999996|\n",
      "|   85123A|WHITE HANGING HEA...|   11|13849.929999999997|\n",
      "|    47566|       PARTY BUNTING|    9| 4386.999999999999|\n",
      "|     POST|             POSTAGE|    2|            3166.0|\n",
      "|        M|              MANUAL|   12|            666.27|\n",
      "|   85123A|WHITE HANGING HEA...|    8| 5498.100000000001|\n",
      "|    84879|ASSORTED COLOUR B...|    3|3997.9799999999996|\n",
      "|    47566|       PARTY BUNTING|    8|           6306.05|\n",
      "|    22423|REGENCY CAKESTAND...|    6|           8216.35|\n",
      "|    84879|ASSORTED COLOUR B...|    5| 4977.239999999999|\n",
      "+---------+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seasonal patterns for top selling products\n",
    "#get products by stock code and their highest total revnue as sum\n",
    "top_products_df = online_retail_df.groupBy(\"StockCode\").agg(F.sum(\"Revenue\").alias(\"TotalRevenue\")).orderBy(F.desc(\"TotalRevenue\"))\n",
    "    \n",
    "#join with df to get montly data, group by product and month for each of their total revenue\n",
    "seasonal_pattern = online_retail_df.join(top_products_df.limit(10), \"StockCode\").groupBy(\"StockCode\", \"Description\", \"Month\").agg(F.sum(\"Revenue\").alias(\"MonthlySales\"))\n",
    "seasonal_pattern.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------+\n",
      "|CustomerID|StockCode|CustomerLifetimeValue|\n",
      "+----------+---------+---------------------+\n",
      "|     15363|    22382|                 16.5|\n",
      "|     17235|   85184C|   35.400000000000006|\n",
      "|     17454|    21931|                 19.5|\n",
      "|     13113|    22423|                700.8|\n",
      "|     14498|    22457|                 11.8|\n",
      "|     15059|    21175|   49.199999999999996|\n",
      "|     13198|    20751|   25.200000000000003|\n",
      "|     16609|    22969|                 34.8|\n",
      "|     15719|    22411|                47.19|\n",
      "|     16992|    22500|                 19.8|\n",
      "|     12668|    23078|                 30.0|\n",
      "|     14298|    22608|               157.32|\n",
      "|     13081|    22132|   30.599999999999998|\n",
      "|     15129|    22360|   35.400000000000006|\n",
      "|     14156|    22113|                  7.5|\n",
      "|     15529|    10002|   15.299999999999999|\n",
      "|     13506|    84077|   13.919999999999998|\n",
      "|     18116|    21381|                 5.07|\n",
      "|     17406|    22795|                 13.5|\n",
      "|     17863|    22560|                 30.0|\n",
      "+----------+---------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#customer liftime value: total revenue per customer\n",
    "clv_df = online_retail_df.groupBy(\"CustomerID\", \"StockCode\").agg(F.sum(\"Revenue\").alias(\"CustomerLifetimeValue\"))\n",
    "clv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|StockCode|PopularityScore|\n",
      "+---------+---------------+\n",
      "|    21889|            449|\n",
      "|    21259|            237|\n",
      "|    22728|            613|\n",
      "|    21452|            133|\n",
      "|    21894|             71|\n",
      "|    22121|            114|\n",
      "|    21248|             52|\n",
      "|    22254|             41|\n",
      "|    21249|             79|\n",
      "|    90143|              7|\n",
      "|    22596|            234|\n",
      "|    84881|              5|\n",
      "|    23318|            329|\n",
      "|    23459|             19|\n",
      "|    21331|              7|\n",
      "|   90210B|              6|\n",
      "|    20868|             31|\n",
      "|    23843|              1|\n",
      "|    22314|             93|\n",
      "|    21535|            310|\n",
      "+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#product popularity: counted by unique transaction made under each stock code\n",
    "product_popularity = online_retail_df.groupBy(\"StockCode\").agg(F.countDistinct(\"InvoiceNo\").alias(\"PopularityScore\"))\n",
    "product_popularity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------------+\n",
      "|StockCode|Season|     SeasonalSales|\n",
      "+---------+------+------------------+\n",
      "|    21110|Winter|            1349.3|\n",
      "|    22668|Winter|            1411.7|\n",
      "|    22966|Winter|2459.8599999999997|\n",
      "|   90002D|Winter|              30.0|\n",
      "|    37446|Winter| 659.5999999999998|\n",
      "|    22252|Spring|             97.94|\n",
      "|    22760|Winter| 849.8999999999999|\n",
      "|   84558A|Winter|268.45000000000005|\n",
      "|    22149|Spring| 2487.700000000001|\n",
      "|    22421|Winter|            159.48|\n",
      "|    22301|Winter|1107.4500000000003|\n",
      "|    22537|Spring|            190.26|\n",
      "|    22107|Winter|            448.74|\n",
      "|    21564|Spring|430.70000000000005|\n",
      "|   84575A|Winter|              5.95|\n",
      "|   84952C|Spring|              30.0|\n",
      "|   15056P|Winter|             708.7|\n",
      "|    22452|Winter|            520.25|\n",
      "|    84818|Winter|             408.0|\n",
      "|    21463|Spring|382.65000000000003|\n",
      "+---------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#seasonal trends:\n",
    "#make season column based on month\n",
    "online_retail_df = online_retail_df.withColumn(\"Season\",\n",
    "                    F.when(col(\"Month\").isin(12, 1, 2), \"Winter\") #if month is in one of these numbers\n",
    "                    .when(col(\"Month\").isin(3, 4, 5), \"Spring\")\n",
    "                    .when(col(\"Month\").isin(6, 7, 8), \"Summer\")\n",
    "                    .when(col(\"Month\").isin(9, 10, 11), \"Fall\")\n",
    "                    )\n",
    "    \n",
    "#total revenue of each product and season\n",
    "seasonal_trends = online_retail_df.groupBy(\"StockCode\", \"Season\").agg(F.sum(\"Revenue\").alias(\"SeasonalSales\"))\n",
    "seasonal_trends.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the aggregated data to the directory as csv files\n",
    "#try:\n",
    "#    total_sales_df.write.csv(task2_output_total, header=True)\n",
    "#    avg_revnue_df.write.csv(task2_output_avgsales, header=True)\n",
    "#    seasonal_pattern.write.csv(task2_output_season, header=True)\n",
    "#except ValueError as e:\n",
    "#    print(f\"error {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join features into a consolidated DataFrame\n",
    "forecasting_df = total_sales_df.join(clv_df, \"StockCode\", \"left\") \\\n",
    "                               .join(product_popularity, \"StockCode\", \"left\") \\\n",
    "                               .join(seasonal_trends, \"StockCode\", \"left\")\n",
    "\n",
    "# Fill nulls with 0 or appropriate values for ML training\n",
    "forecasting_df = forecasting_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----+----+----------+----------+---------------------+---------------+------+-----------------+-----------+------------+\n",
      "|StockCode|         Description|Month|Year|TotalSales|CustomerID|CustomerLifetimeValue|PopularityScore|Season|    SeasonalSales|Lag_1_Month|Lag_2_Months|\n",
      "+---------+--------------------+-----+----+----------+----------+---------------------+---------------+------+-----------------+-----------+------------+\n",
      "|   10123C|HEARTS WRAPPING T...|   12|2010|      0.65|     17967|                 0.65|              3|Winter|             0.65|       0.65|        0.65|\n",
      "|   10123C|HEARTS WRAPPING T...|   12|2010|      0.65|     17967|                 0.65|              3|Spring|              2.6|       0.65|        0.65|\n",
      "|   10123C|HEARTS WRAPPING T...|   12|2010|      0.65|     14670|   1.9500000000000002|              3|Winter|             0.65|       0.65|        0.65|\n",
      "|   10123C|HEARTS WRAPPING T...|   12|2010|      0.65|     14670|   1.9500000000000002|              3|Spring|              2.6|       0.65|        0.65|\n",
      "|   10123C|HEARTS WRAPPING T...|    3|2011|       2.6|     14064|                 0.65|              3|Winter|             0.65|       0.65|        0.65|\n",
      "|   10123C|HEARTS WRAPPING T...|    3|2011|       2.6|     14064|                 0.65|              3|Spring|              2.6|        2.6|        0.65|\n",
      "|   10123C|HEARTS WRAPPING T...|    3|2011|       2.6|     17967|                 0.65|              3|Winter|             0.65|        2.6|         2.6|\n",
      "|   10123C|HEARTS WRAPPING T...|    3|2011|       2.6|     17967|                 0.65|              3|Spring|              2.6|        2.6|         2.6|\n",
      "|   10123C|HEARTS WRAPPING T...|    3|2011|       2.6|     14670|   1.9500000000000002|              3|Winter|             0.65|        2.6|         2.6|\n",
      "|   10123C|HEARTS WRAPPING T...|    3|2011|       2.6|     14670|   1.9500000000000002|              3|Spring|              2.6|        2.6|         2.6|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     16841|                  8.4|            122|Summer|743.2800000000001|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     16841|                  8.4|            122|Winter|            152.3|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     16706|                 0.84|            122|  Fall|            81.06|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     16706|                 0.84|            122|Spring|           162.35|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     16706|                 0.84|            122|Summer|743.2800000000001|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     16706|                 0.84|            122|Winter|            152.3|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     14132|   50.400000000000006|            122|  Fall|            81.06|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     14132|   50.400000000000006|            122|Spring|           162.35|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     14132|   50.400000000000006|            122|Summer|743.2800000000001|      78.75|       78.75|\n",
      "|    10133|COLOURING PENCILS...|   12|2010|     78.75|     14132|   50.400000000000006|            122|Winter|            152.3|      78.75|       78.75|\n",
      "+---------+--------------------+-----+----+----------+----------+---------------------+---------------+------+-----------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create lag features using window functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy(\"StockCode\").orderBy(\"Year\", \"Month\")\n",
    "forecasting_df = forecasting_df.withColumn(\"Lag_1_Month\", F.lag(\"TotalSales\", 1).over(windowSpec))\n",
    "forecasting_df = forecasting_df.withColumn(\"Lag_2_Months\", F.lag(\"TotalSales\", 2).over(windowSpec))\n",
    "\n",
    "# Drop rows with null values created by lags\n",
    "forecasting_df = forecasting_df.na.drop()\n",
    "\n",
    "# Display the consolidated DataFrame structure\n",
    "forecasting_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble feature columns\n",
    "feature_cols = [\"Lag_1_Month\", \"Lag_2_Months\", \"CustomerLifetimeValue\", \"PopularityScore\", \"SeasonalSales\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Initialize the regression model (e.g., Linear Regression)\n",
    "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"TotalSales\")\n",
    "\n",
    "# Create a pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 18:16:46 ERROR Executor: Exception in task 0.0 in stage 110.0 (TID 165)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getIterator(UnsafeExternalSorter.java:766)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:183)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:187)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:160)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:180)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda/0x000073573d18a6d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x000073573cebfb00.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "24/11/19 18:16:46 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#72,Executor task launch worker for task 0.0 in stage 110.0 (TID 165),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getIterator(UnsafeExternalSorter.java:766)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:183)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:187)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:160)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:180)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda/0x000073573d18a6d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x000073573cebfb00.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "24/11/19 18:16:46 WARN TaskSetManager: Lost task 0.0 in stage 110.0 (TID 165) (cbfbc0c8-dbcb-450c-ac3d-921bbc3fdee1.internal.cloudapp.net executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getIterator(UnsafeExternalSorter.java:766)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:183)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:187)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:160)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:180)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda/0x000073573d18a6d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x000073573cebfb00.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "24/11/19 18:16:46 ERROR TaskSetManager: Task 0 in stage 110.0 failed 1 times; aborting job\n",
      "24/11/19 18:16:46 WARN TaskSetManager: Lost task 2.0 in stage 110.0 (TID 167) (cbfbc0c8-dbcb-450c-ac3d-921bbc3fdee1.internal.cloudapp.net executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 110.0 failed 1 times, most recent failure: Lost task 0.0 in stage 110.0 (TID 165) (cbfbc0c8-dbcb-450c-ac3d-921bbc3fdee1.internal.cloudapp.net executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getIterator(UnsafeExternalSorter.java:766)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:183)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:187)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:160)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:180)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda/0x000073573d18a6d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x000073573cebfb00.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/11/19 18:16:46 WARN TaskSetManager: Lost task 1.0 in stage 110.0 (TID 166) (cbfbc0c8-dbcb-450c-ac3d-921bbc3fdee1.internal.cloudapp.net executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 110.0 failed 1 times, most recent failure: Lost task 0.0 in stage 110.0 (TID 165) (cbfbc0c8-dbcb-450c-ac3d-921bbc3fdee1.internal.cloudapp.net executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getIterator(UnsafeExternalSorter.java:766)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:183)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.generateIterator(ExternalAppendOnlyUnsafeRowArray.scala:187)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:160)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:180)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda/0x000073573d18a6d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x000073573cebfb00.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "Driver stacktrace:)\n",
      "ERROR:root:Exception while sending command.                         (0 + 1) / 4]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_9828/3880481004.py\", line 5, in <module>\n",
      "    model = pipeline.fit(train_data)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "train_data, test_data = forecasting_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"TotalSales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
